{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146e7a00-dfac-4c46-919d-1209d2dbf886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa293aa-c13a-43a0-9de1-2c35367b524b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prophet in /databricks/python3/lib/python3.12/site-packages (1.1.5)\nRequirement already satisfied: cmdstanpy>=1.0.4 in /databricks/python3/lib/python3.12/site-packages (from prophet) (1.2.5)\nRequirement already satisfied: numpy>=1.15.4 in /databricks/python3/lib/python3.12/site-packages (from prophet) (1.26.4)\nRequirement already satisfied: matplotlib>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from prophet) (3.8.4)\nRequirement already satisfied: pandas>=1.0.4 in /databricks/python3/lib/python3.12/site-packages (from prophet) (1.5.3)\nRequirement already satisfied: holidays>=0.25 in /databricks/python3/lib/python3.12/site-packages (from prophet) (0.54)\nRequirement already satisfied: tqdm>=4.36.1 in /databricks/python3/lib/python3.12/site-packages (from prophet) (4.66.4)\nRequirement already satisfied: importlib-resources in /databricks/python3/lib/python3.12/site-packages (from prophet) (6.5.2)\nRequirement already satisfied: stanio<2.0.0,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.12/site-packages (from holidays>=0.25->prophet) (2.9.0.post0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (24.1)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.4->prophet) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil->holidays>=0.25->prophet) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: mlflow in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c14a1709-ce39-4ab7-a659-8720d93eda94/lib/python3.12/site-packages (2.20.3)\nRequirement already satisfied: mlflow-skinny==2.20.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c14a1709-ce39-4ab7-a659-8720d93eda94/lib/python3.12/site-packages (from mlflow) (2.20.3)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.2.5)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (3.1.4)\nRequirement already satisfied: alembic!=1.10.0,<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.14.0)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c14a1709-ce39-4ab7-a659-8720d93eda94/lib/python3.12/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c14a1709-ce39-4ab7-a659-8720d93eda94/lib/python3.12/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (3.4.1)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (3.8.4)\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (15.0.2)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.4.2)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.13.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.0.30)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (2.2.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (0.30.0)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.37)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (1.29.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (1.29.0)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (24.1)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (4.24.1)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (2.8.2)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (6.0.1)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (2.32.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (0.4.2)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==2.20.3->mlflow) (4.11.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.0)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.16)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.0.3)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.4)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c14a1709-ce39-4ab7-a659-8720d93eda94/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.12/dist-packages (from gunicorn<24->mlflow) (74.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.1)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (2.21.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.3->mlflow) (3.17.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.2.15)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (0.50b0)\nRequirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (2.20.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (2024.6.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (4.9)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.4.8)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install prophet\n",
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e757135-ec46-433f-bf38-caaf30ba4099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4087f5-e87f-4d0d-8ecf-225ffdbc8c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import prophet\n",
    "import mlflow\n",
    "import datetime\n",
    "import pyspark\n",
    "import itertools\n",
    "from pyspark.sql import SparkSession\n",
    "from prophet import Prophet, serialize\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import (\n",
    "    MonthLocator,\n",
    "    num2date,\n",
    "    AutoDateLocator,\n",
    "    AutoDateFormatter,\n",
    ")\n",
    "from pyspark.sql.functions import pandas_udf, ceil, when, lit\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    ")\n",
    "import logging\n",
    "import json\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcced34-e2ee-4718-bb1d-58cd2f2e7e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Get Customer Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b17b78-9a96-4876-8056-77291bfaaba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_customer_volume_data(\n",
    "    start_date: str, end_date: str, date_column: str\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"Get daily customer count data for deposit accounts\"\"\"\n",
    "    return spark.sql(f\"select * from discovery.new_customer_counts_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0576197c-6e37-4eb4-91a7-f74abce6cd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Fit Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf4fc92-682a-418a-81f9-cd5725997f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fit_new_prophet_model(\n",
    "    df: pd.DataFrame,\n",
    "    tuned_params: dict,\n",
    ") -> Prophet:\n",
    "    \"\"\"Takes a dataframe, institution id and feature label (long-format df) and returns a trained prophet model\"\"\"\n",
    "    model = Prophet(**tuned_params)\n",
    "    model.add_seasonality(name=\"monthly\", period=30.5, fourier_order=5)\n",
    "    return model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c119d3d-fdde-4aa1-b07f-6e7e8f36921f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Fit Prophet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d83fec-d4af-4de5-a6fc-a85e547bf195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_params(pr_model) -> dict:\n",
    "    \"\"\"Get parameters of a prophet model\"\"\"\n",
    "    return {attr: getattr(pr_model, attr) for attr in serialize.SIMPLE_ATTRIBUTES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753ec7d1-1e3b-4829-819a-dd0dc6296449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Process Data Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e1d38a-e53a-4b8f-80ee-a004c4860064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_path_and_colnames(\n",
    "    df: pd.DataFrame, client_name_colname: str\n",
    ") -> (pd.DataFrame, str):\n",
    "    \"\"\"Strip leading/trailing whitespace from Institution name column\"\"\"\n",
    "    df[client_name_colname] = df[client_name_colname].str.strip()\n",
    "    return (\n",
    "        df,\n",
    "        df[client_name_colname].head(1).to_numpy()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4308ebd6-d7d9-4652-b31f-78db36a04e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Create Prophet Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a44037c-2b06-421f-90cb-29d56a1d21f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_prophet_input(\n",
    "    df: pd.DataFrame, date_column: str, feature_value_column: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Transform forecast input from data to prophet forecast input\"\"\"\n",
    "    return (\n",
    "        df.rename(columns={date_column: \"ds\", feature_value_column: \"y\"})[[\"ds\", \"y\"]]\n",
    "        .groupby(\"ds\")\n",
    "        .agg({\"ds\": \"first\", \"y\": \"sum\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a470e4-af74-45eb-b37a-7c8b9f3c43a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Get Training Data Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb699f5c-ac77-4927-ba7f-93aabc7cceae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_training_data_metrics(df, training_data_column: str = \"y\"):\n",
    "    \"\"\"Get some training data metrics: unique, min, max, mean, sd, range\"\"\"\n",
    "    min_training_value = df[training_data_column].min()\n",
    "    max_training_value = df[training_data_column].max()\n",
    "    return (\n",
    "        df[training_data_column].nunique(),\n",
    "        min_training_value,\n",
    "        max_training_value,\n",
    "        pd.to_numeric(df[training_data_column]).mean(),\n",
    "        pd.to_numeric(df[training_data_column]).std(),\n",
    "        max_training_value - min_training_value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04b7f42-795b-4521-90fe-5953fbaf1049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Functions: Hyperparameter Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4ce5fa-0d94-4a63-9c9b-4ac1375ca889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_prophet_hyperparam_grid(\n",
    "    hyperparamter_grid: dict = {\n",
    "        \"changepoint_prior_scale\": [0.05, 0.1, 0.5],\n",
    "        \"seasonality_prior_scale\": [0.01, 0.1, 1.0, 10.0],\n",
    "        \"seasonality_mode\": [\"additive\", \"multiplicative\"],\n",
    "    }\n",
    ") -> dict:\n",
    "    \"\"\"Define hyperparameter grid\"\"\"\n",
    "    return hyperparamter_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018b0500-7fe2-4bfc-be87-5b9763f28a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_prophet_hyperparam_powerset(hyperparamter_grid: dict):\n",
    "    \"\"\"Generate all combinations of hyperparameters to iterate over\"\"\"\n",
    "    return [\n",
    "        dict(zip(hyperparamter_grid.keys(), v))\n",
    "        for v in itertools.product(*hyperparamter_grid.values())\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67afe03-62cf-40d3-8097-b9adfbefc3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_hyperparams_grid_search(\n",
    "    df: pd.DataFrame,\n",
    "    all_params: dict,\n",
    "    cv_horizon: str = \"21 days\",\n",
    "    cv_period: str = None,\n",
    "    cv_initial: str = None,\n",
    ") -> dict:\n",
    "    \"\"\"Use cross validation to evaluate all hyperparameter combinations (tuned_params),\n",
    "    pick the top performing model based on RMSE, and reeturn those hyperparameters\"\"\"\n",
    "    df = df.copy()\n",
    "    rmses = []\n",
    "    for params in all_params:\n",
    "        m = Prophet(**params)\n",
    "        m.add_seasonality(name=\"monthly\", period=30.5, fourier_order=5)\n",
    "        m.fit(df)\n",
    "        df_cv = cross_validation(\n",
    "            m,\n",
    "            horizon=cv_horizon,\n",
    "            period=cv_period,\n",
    "            initial=cv_initial,\n",
    "            parallel=\"processes\",\n",
    "        )\n",
    "        performance_df = performance_metrics(df_cv, rolling_window=1)\n",
    "        rmses.append(performance_df[\"rmse\"].values[0])\n",
    "    # Get the best parameters\n",
    "    results_df = pd.DataFrame(all_params)\n",
    "    results_df[\"rmse\"] = rmses\n",
    "    results_df = results_df.sort_values(\"rmse\", ascending=True)\n",
    "    return results_df[list(all_params[0].keys())].iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84545098-1d3e-431c-a866-21fc4427207d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Future Forecast Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf6373f-f605-4d41-859f-877087d45242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_future_forecast_dataframe(\n",
    "    m: Prophet,\n",
    "    horizon: int,\n",
    "    date_column: str,\n",
    "    client_name: str,\n",
    "    client_name_column: str = \"TradingName\",\n",
    "    prophet_date_column: str = \"ds\",\n",
    "    date_format: str = \"%Y-%m-%d\",\n",
    "):\n",
    "    \"\"\"Get forecast future dataframe\"\"\"\n",
    "    df = m.predict(m.make_future_dataframe(periods=horizon))\n",
    "    df[date_column] = df[prophet_date_column].dt.strftime(date_format)\n",
    "    df[client_name_column] = client_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a632a94-7635-4c58-9b51-c9914c68ca4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Validate and get Forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c90767-258f-4563-8519-fee3f2dfb3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_and_get_forecast_metrics(\n",
    "    m: Prophet,\n",
    "    training_data_metrics: tuple,\n",
    "    cv_horizon: str = \"21 days\",\n",
    "    cv_period: str = None,\n",
    "    cv_initial: str = None,\n",
    "    metric_keys: list = [\"mse\", \"rmse\", \"mae\", \"mape\", \"mdape\", \"coverage\"],\n",
    "    training_metrics_keys: list = [\n",
    "        \"unique_training_values\",\n",
    "        \"min_training_value\",\n",
    "        \"max_training_value\",\n",
    "        \"mean_training_value\",\n",
    "        \"stddev_training_value\",\n",
    "        \"range_training_values\",\n",
    "    ],\n",
    ") -> (pd.DataFrame, dict):\n",
    "    \"\"\"Get cross validation and training data metrics\"\"\"\n",
    "    metrics_raw = cross_validation(\n",
    "        model=m,\n",
    "        horizon=cv_horizon,\n",
    "    )\n",
    "    cv_metrics = performance_metrics(metrics_raw)\n",
    "    metrics = {**{k: cv_metrics[k].mean() for k in metric_keys}}\n",
    "    for i, training_metrics_key in enumerate(training_metrics_keys):\n",
    "        metrics[training_metrics_key] = training_data_metrics[i]\n",
    "    return metrics_raw, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc982d61-bb16-4d02-bc8d-5678f00e1136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Log outputs & Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31d8888-967d-4b83-88b3-a5b044a2ae3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_forecast_and_log_outputs(\n",
    "    df: pd.DataFrame,\n",
    "    m: Prophet,\n",
    "    metrics_raw: pd.DataFrame,\n",
    "    artifact_path: str,\n",
    "    metrics: dict,\n",
    "    params: dict,\n",
    "    forecast_plot_filename: str = \"forecast.png\",\n",
    "    components_plot_filename: str = \"components.png\",\n",
    "    cross_validation_plot_filename: str = \"cross_validation.png\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get forecast data and log figures, params and metrics pertaining to them\"\"\"\n",
    "    df = df.copy()\n",
    "    forecast_df = m.predict(df)\n",
    "    mlflow.log_figure(m.plot(forecast_df), forecast_plot_filename)\n",
    "    mlflow.log_figure(m.plot_components(forecast_df), components_plot_filename)\n",
    "    mlflow.log_figure(\n",
    "        plot_cross_validation_metric(metrics_raw, metric=\"mape\"),\n",
    "        cross_validation_plot_filename,\n",
    "    )\n",
    "    mlflow.prophet.log_model(m, artifact_path=artifact_path)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5784a2b-73b9-403e-9c21-50bfbe91f1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_experiment_output(\n",
    "    df: pd.DataFrame,\n",
    "    m: Prophet,\n",
    "    run_id: str,\n",
    "    forecast_label: str,\n",
    "    client_name: str,\n",
    "    unique_training_values: int,\n",
    "    forecast_label_header: str = \"forecast_label\",\n",
    "    client_name_header: str = \"TradingName\",\n",
    "    unique_training_header: str = \"UniqueTrainingValues\",\n",
    "    run_id_header: str = \"RunId\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add output columns for table to forecast output in UDF\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"y\"] = m.history[\"y\"]\n",
    "    df[forecast_label_header] = forecast_label\n",
    "    df[client_name_header] = client_name\n",
    "    df[unique_training_header] = unique_training_values\n",
    "    df[run_id_header] = run_id\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e26062-d372-410b-ae43-9e158dae999a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function: Train and Optimise Prophet Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da8ea09-a4a8-45dd-aa2f-4838eb075613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_train_and_evaluate_prophet(\n",
    "    train_start: str,\n",
    "    train_end: str,\n",
    "    horizon: int,\n",
    "    forecast_label: str,\n",
    "    output_columns: list = [\n",
    "        \"forecast_label\",\n",
    "        \"TradingName\",\n",
    "        \"ds\",\n",
    "        \"y\",\n",
    "        \"yhat\",\n",
    "        \"yhat_lower\",\n",
    "        \"yhat_upper\",\n",
    "        \"UniqueTrainingValues\",\n",
    "        \"RunId\",\n",
    "    ],\n",
    "    client_name_colname: str = \"TradingName\",\n",
    "    date_column: str = \"Date\",\n",
    ") -> pd.DataFrame:\n",
    "    def train_and_evaluate_prophet(df):\n",
    "        \"\"\"Train prophet model based on date-value pair dataframe\"\"\"\n",
    "        df, client_name = get_path_and_colnames(\n",
    "            df, client_name_colname,\n",
    "        )\n",
    "        artifact_path = f\"{forecast_label}_{client_name}_{train_start}_{train_end}_{horizon}\"\n",
    "        df = get_prophet_input(df, date_column, forecast_label)\n",
    "        with mlflow.start_run(\n",
    "            run_name=artifact_path,\n",
    "            description=f\"A Prophet forecasting run for {artifact_path}\\n\",\n",
    "        ) as run:\n",
    "            try:\n",
    "                training_data_metrics = get_training_data_metrics(df)\n",
    "                if training_data_metrics[0] > 10 and df.shape[0] > 100:\n",
    "                    all_hyperparams = get_prophet_hyperparam_powerset(\n",
    "                        get_prophet_hyperparam_grid()\n",
    "                    )\n",
    "                    tuned_hyperparams = run_hyperparams_grid_search(df, all_hyperparams)\n",
    "                else:\n",
    "                    tuned_hyperparams = {\"changepoint_prior_scale\": 0.05}\n",
    "\n",
    "                m = fit_new_prophet_model(\n",
    "                    df,\n",
    "                    tuned_hyperparams,\n",
    "                )\n",
    "                params = extract_params(m)\n",
    "                future_df = create_future_forecast_dataframe(\n",
    "                    m, horizon, date_column, client_name\n",
    "                )\n",
    "                metrics_raw, metrics = validate_and_get_forecast_metrics(\n",
    "                    m, training_data_metrics\n",
    "                )\n",
    "                forecast_df = get_forecast_and_log_outputs(\n",
    "                     future_df, m, metrics_raw, artifact_path, metrics, params\n",
    "                )\n",
    "                forecast_df = generate_experiment_output(\n",
    "                    forecast_df,\n",
    "                    m,\n",
    "                    run.info.run_id,\n",
    "                    forecast_label,\n",
    "                    client_name,\n",
    "                    training_data_metrics[0],\n",
    "                )\n",
    "                return forecast_df[output_columns]\n",
    "            except Exception as e:\n",
    "                mlflow.set_tag(\"exception\", \"True\")\n",
    "                mlflow.set_tag(\"error_message\", e)\n",
    "                return pd.DataFrame(columns=output_columns)\n",
    "\n",
    "    return train_and_evaluate_prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1351be44-c0af-491f-aae1-f7e4a85e04e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run Prophet Pipeline In PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b48c18-8baf-46fe-a6a0-7b5769471a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_forecast_pipeline(\n",
    "    df: pyspark.sql.DataFrame,\n",
    "    aggregation_columns: list,\n",
    "    train_start: str,\n",
    "    train_end: str,\n",
    "    horizon_days: int,\n",
    "    forecast_label: str,\n",
    "    data_layer: str = \"discovery\",\n",
    "    client_name_column: str = \"TradingName\",\n",
    "    output_table: str = \"customer_forecast_demo\",\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    spark = SparkSession.builder.appName(f\"{forecast_label}_run\").getOrCreate()\n",
    "    forecast_schema = StructType(\n",
    "        [\n",
    "            StructField(\"forecast_label\", StringType(), True),\n",
    "            StructField(client_name_column, StringType(), True),\n",
    "            StructField(\"ds\", TimestampType(), True),\n",
    "            StructField(\"y\", DoubleType(), True),\n",
    "            StructField(\"yhat\", DoubleType(), True),\n",
    "            StructField(\"yhat_lower\", DoubleType(), True),\n",
    "            StructField(\"yhat_upper\", DoubleType(), True),\n",
    "            StructField(\"UniqueTrainingValues\", IntegerType(), True),\n",
    "            StructField(\"RunId\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    forecasts_df = (\n",
    "        df.groupBy(aggregation_columns)\n",
    "        .applyInPandas(\n",
    "            create_train_and_evaluate_prophet(\n",
    "                train_start,\n",
    "                train_end,\n",
    "                horizon_days,\n",
    "                forecast_label\n",
    "            ),\n",
    "            schema=forecast_schema,\n",
    "        )\n",
    "    )\n",
    "    forecasts_df.write.mode(\"overwrite\").saveAsTable(f\"{data_layer}.{output_table}\")\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6c20c4-d127-4418-a1bc-f29ab8044886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df4415f-a7cb-4cfd-98f7-58940df861e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[forecast_label: string, TradingName: string, ds: timestamp, y: double, yhat: double, yhat_lower: double, yhat_upper: double, UniqueTrainingValues: int, RunId: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = \"2024-12-03\"\n",
    "horizon_days = 90\n",
    "train_days = 365\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "train_end = datetime.date.today().strftime(date_format)\n",
    "train_start = (datetime.datetime.strptime(current_date, date_format) - datetime.timedelta(days=train_days)).strftime(\n",
    "    date_format\n",
    ")\n",
    "\n",
    "customer_volume_df = get_customer_volume_data(train_start, train_end, \"Date\")\n",
    "\n",
    "run_forecast_pipeline(\n",
    "    customer_volume_df,\n",
    "    [\"TradingName\"],\n",
    "    train_start,\n",
    "    train_end,\n",
    "    horizon_days,\n",
    "    \"NewCustomers\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "forecasting_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}